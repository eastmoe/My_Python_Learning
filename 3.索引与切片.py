import time
import torch

print('程序开始时间：',time.strftime('%Y-%m-%d %H:%M:%S')) #记录程序开始时间

a = torch.rand(120,3,512,512,)  # 生成120*3*512*512的四维张量
a = a.cuda()  # 把a搬移到GPU
print('新张量a的数据类型',a.type())  # 输出a的数据类型
print('新张量a的形状',a.shape)  # 输出a的形状
print('使用一个维度索引：',a[0].shape)  # 索引指定第一个维度的内容。pytorch默认从左边开始索引。
print('使用两个维度索引：',a[0,0].shape)  # 索引指定第一个维度的第一项内容
print('直接索引某个元素：',a[0,0,2,4],)  # 索引指定第一个维度的第一项内容的第二行第四列。这是一个维度为0的标量。

print('---------------------------------------------------------------------------------------------------------------')

# : 取全部
# :n 从第一个取到n（不包括n）
# n: 从n取到最后（包括n）
# m:n 从m到n（不包括n）
print('取出来的一维的0-2的形状：',a[:2].shape)  # 从一维的0到一维的2（不包括2）的数据，可以把“:”理解为“->”。
print('取出来的一维的0-2，二维的0-1的形状：',a[:2,:1].shape)  # 从一维的0到2（不包括2），从二维的0到1（不含1）的数据，可以把“:”理解为“->”。
print('取出来的一维的0-2，二维的1-2的形状：',a[:2,1:].shape)  # 从一维的0到2（不包括2），从二维的1到2（含1、2）的数据，可以把“:”理解为“->”。
print('取出来的一维的0-2，二维的倒数第一的形状：',a[:2,-1:].shape)  # 从一维的0到2（不包括2），从二维的倒数第一（含倒数第一）的数据，可以把“:”理解为“->”。

print('---------------------------------------------------------------------------------------------------------------')

# m:n:k 从m到n（不包括n）间隔k采样
# ::k 从开始到结束，间隔k采样
print('保留一维和二维，三、四维间隔2采样：',a[:,:,0:512:2,0:512:2].shape)  # 前两维不变，后面的从0到512（不含）间隔2采样
print('保留一维和二维，三、四维间隔2采样：',a[:,:,::2,::2].shape)  # 前两维不变，后面的从开始到结束间隔2采样

print('---------------------------------------------------------------------------------------------------------------')

a_02sel = a.index_select(0,torch.tensor([0,2]).cuda())  # 抽取第一个维度上的0、2项
a_02sel = a_02sel.cuda()  #搬运到GPU
print('抽取第一维的0、2项：',a_02sel.shape)  #输出抽取的形状
a_21sel = a.index_select(1,torch.tensor([1,2]).cuda())  # 抽取第二个维度上的1、2项
a_21sel = a_21sel.cuda()  #搬运到GPU
print('抽取第二维的1、2项：',a_21sel.shape)  #输出抽取的形状
a_17sel = a.index_select(2,torch.arange(8).cuda())# 抽取第三个维度上的0-8（不含8）项
a_17sel = a_17sel.cuda()  #搬运到GPU
print('抽取第三维的0-7项：',a_17sel.shape)  #输出抽取的形状
# 注意，使用index_selection时，第二个参数（抽取的项）必须要使用tensor（张量）格式，而不能使用python的列表（list）类型
print('---------------------------------------------------------------------------------------------------------------')

print('本身：',a[...].shape)  # a[...]=a，此处仅仅为了举例
print('取第一维的所有：',a[0,...].shape)  # =a[0,:,:,:],等同于直接切去第一个维度
print('取第二维的前2项：',a[:,:2,...].shape)  # =a[:,:2,:,:],其它维不变，仅从第二维中取前两项和对应的其它维
print('取第四维的前4项：',a[...,:4].shape)  # =a[:,:,:,:2],取第四维（最后一维的前四项
# ）
print('---------------------------------------------------------------------------------------------------------------')

b = torch.rand(4,5)  # 生成4*5的二维随机数张量b
b = b.cuda()  # 把b搬运到cuda
b_mask1 = b.ge(0.75)  # 计算b大于0.75的元素并且把这些元素的掩码标注为1
b_mask1 = b_mask1.cuda()  # 把b_mask1搬运到GPU
print('张量b：',b)  # 输出原本的张量b
print('张量b以0.75为单位的掩码：',b_mask1)  # 输出b对应于0.75的掩码
b_075sel = torch.masked_select(b,b_mask1)  # 将这些掩码为1（即大于0.75）的元素抽出
b_075sel = b_075sel.cuda()  # 把抽出结果搬运到GPU
print('掩码为1的元素：',b_075sel)  # 输出抽出结果
print('掩码为1的元素组成的张量形状：',b_075sel.shape)  # 计算抽出张量的形状




print('---------------------------------------------------------------------------------------------------------------')

print('张量b：',b)  # 输出原本的张量b
b_take = torch.take(b,torch.tensor([1,4]).cuda())  # 将b打平，然后取第2、5个
b_take = b_take.cuda()  # 把b_take搬运到GPU
print('b打平后编号为2、5的数据：',b_take)

print('---------------------------------------------------------------------------------------------------------------')

print('程序结束时间：',time.strftime('%Y-%m-%d %H:%M:%S'))  # 记录程序结束时间