import time
import torch

print('程序开始时间：',time.strftime('%Y-%m-%d %H:%M:%S')) #记录程序开始时间

print('---------------------------------------------------------------------------------------------------------------')
# 以下是张量内元素的加减乘除，而不是张量矩阵的运算。
a = torch.rand(4,4).cuda()  # 生成2维4*4张量a
b = torch.rand(4,4).cuda()  # 生成2维4*4张量a
print('张量a：',a)  # 输出原先的张量a
print('张量b：',b)  # 输出原先的张量b
a_b_1 = torch.add(a,b).cuda()  # 利用torch计算加减乘除
print('用torch相加a和b：',a_b_1)  # 输出计算结果
a_b_2 = a+b.cuda()  # 直接利用符号计算加减乘除
print('用加号相加a和b：',a_b_2)  # 输出计算结果
print('对比计算结果是否一致：',torch.all(torch.eq(a_b_1,a_b_2)))  # 比较两种方式的计算结果是否完全相同
print('')  # 输出空白行

a2 = torch.rand(4,4).cuda()  # 生成2维4*4张量a2
b2 = torch.rand(4).cuda()  # 生成1维张量a2
print('张量a2：',a2)  # 输出原先的张量a2
print('张量b2：',b2)  # 输出原先的张量b2
a2_b2_1 = torch.add(a2,b2).cuda()  # 利用torch计算加减乘除（原先的张量维度会自动扩展）
print('用torch计算a2+b2：',a2_b2_1)  # 输出计算结果
a2_b2_2 = a2+b2.cuda()  # 直接利用符号计算加减乘除
print('用符号计算a2+b2：',a2_b2_2)  # 输出计算结果
print('对比计算结果是否一致：',torch.all(torch.eq(a2_b2_1,a2_b2_2)))  # 比较两种方式的计算结果是否完全相同
print('')  # 输出空白行

a2_b2_sub_1 = torch.sub(a2,b2).cuda()  # 利用torch计算加减乘除（原先的张量维度会自动扩展）
print('用torch计算a2-b2：',a2_b2_sub_1)  # 输出计算结果
a2_b2_sub_2 = a2-b2.cuda()  # 直接利用符号计算加减乘除
print('用符号计算a2-b2：',a2_b2_sub_2)  # 输出计算结果
print('对比计算结果是否一致：',torch.all(torch.eq(a2_b2_sub_1,a2_b2_sub_2)))  # 比较两种方式的计算结果是否完全相同
print('')  # 输出空白行

a2_b2_mul_1 = torch.mul(a2,b2).cuda()  # 利用torch计算加减乘除（原先的张量维度会自动扩展）
print('用torch计算a2*b2：',a2_b2_mul_1)  # 输出计算结果
a2_b2_mul_2 = a2*b2.cuda()  # 直接利用符号计算加减乘除
print('用符号计算a2*b2：',a2_b2_mul_2)  # 输出计算结果
print('对比计算结果是否一致：',torch.all(torch.eq(a2_b2_mul_1,a2_b2_mul_2)))  # 比较两种方式的计算结果是否完全相同
print('')  # 输出空白行

a2_b2_div_1 = torch.div(a2,b2).cuda()  # 利用torch计算加减乘除（原先的张量维度会自动扩展）
print('用torch计算a2/b2：',a2_b2_div_1)  # 输出计算结果
a2_b2_div_2 = a2/b2.cuda()  # 直接利用符号计算加减乘除
print('用符号计算a2/b2：',a2_b2_div_2)  # 输出计算结果
print('对比计算结果是否一致：',torch.all(torch.eq(a2_b2_div_1,a2_b2_div_2)))  # 比较两种方式的计算结果是否完全相同

print('---------------------------------------------------------------------------------------------------------------')

print('张量a：',a)  # 输出原先的张量a
print('张量b：',b)  # 输出原先的张量b
print('a与b矩阵相乘的方法一结果：',torch.mm(a,b).cuda())  # mm只适用于二维矩阵张量相乘，一般不推荐
print('a与b矩阵相乘的方法二结果：',torch.matmul(a,b).cuda())  # torch的通用乘法
print('a与b矩阵相乘的方法三结果：',a@b.cuda())  # 使用符号运算矩阵相乘，结果与上面一致



print('---------------------------------------------------------------------------------------------------------------')

# 这就是简单的线性层实例：xw+b --> x@w.t()+b

x = torch.rand(4,784).cuda()  # 实例，张量里有4张照片，第二个维度代表打平的784个像素
print('原数据张量x的形状：',x.shape)
w = torch.rand(512,784).cuda()  # w张量负责将第二个维度的784个项目降成512个项目
print('转置张量w的形状：',w.shape)
x_out = (x@w.t()).cuda()  # x和转置之后的w相乘，对w进行转置，是为了进行矩阵乘法时能够对应上。
# 注意，.t()只适用于二维的张量进行转置，三维及之上的需要使用.transpose()
print('变换后新的x的形状：',x_out.shape)


print('---------------------------------------------------------------------------------------------------------------')

a3 = torch.rand(2,4,32,28).cuda()  # 生成四维张量a3
b3 = torch.rand(2,4,28,14).cuda()  # 生成四维张量b3
print('a3的形状：',a3.shape)  # 输出a3的形状
print('b3的形状：',b3.shape)  # 输出b3的形状
a3_m_b3 = torch.matmul(a3,b3).cuda()  # 对a3和b3做乘法，不能使用mm。多维张量相乘，只计算最后的两个维度的矩阵乘法，前面的保持不变。
print('a3与b3相乘后的形状：',a3_m_b3.shape)  #输出计算结果
print('')
b4 = torch.rand(2,1,28,14).cuda()  # 生成四维张量b4
print('a3的形状：',a3.shape)  # 输出a3的形状
print('b4的形状：',b4.shape)  # 输出b4的形状
a3_m_b4 = torch.matmul(a3,b4).cuda()  # 对a3和b4做乘法，这里第两个维度不同，但是因为二维是1，可以使用bruadcast自动扩展机制补全。
print('a3与b4相乘后的形状：',a3_m_b4.shape)  #输出计算结果

print('---------------------------------------------------------------------------------------------------------------')

c = torch.full([3,3],4).cuda()  # 生成元素全部为4的三阶二维张量
print('3阶矩阵张量c:',c)  # 输出c
c_power_2 = c.pow(2).cuda()  # 计算c的二次方
print('c的2次方：',c_power_2)  # 输出计算结果
c_power_3 = (c**3).cuda()  # 计算c的三次方，**表示次幂。
print('c的3次方(符号计算法)：',c_power_3)  # 输出计算结果
c_sq = c.sqrt().cuda()  # 计算c的平方根
print('c的平方根：',c_sq)  # 输出结果
c_sq_2 = (c**(0.25)).cuda()  # 计算c的4次方根（0.25次方），使用**符号。
print('c的4次方根（符号法）：',c_sq_2)  # 输出结果

print('---------------------------------------------------------------------------------------------------------------')

d = torch.ones(2,2).cuda()  # 生成二维2*2，元素全部为1的张量
print('2*2的全1张量：',d)  # 输出d
d_e = torch.exp(d).cuda()  # exp（n），指输出的张量里的每一个元素都是e的原本元素次方，即以e为底，原矩阵张量中的元素为幂。
print('经过e变换：',d_e)  # 输出全e矩阵
d_el = torch.log(d_e).cuda()  # 对上一步的张量取以e为底的对数。
print('取自然对数',d_el)

print('---------------------------------------------------------------------------------------------------------------')

e = torch.tensor(3.1415926).cuda()
print('输出Π的前几位：',e)
print('对Π向下取整数：',e.floor())
print('对Π向上取整数：',e.ceil())
print('对Π通过裁剪取整数部分：',e.trunc())
print('对Π通过裁剪取小数部分：',e.frac())
print('对Π通过裁剪取小数部分：',e.frac())
print('')
f = torch.tensor(3.56).cuda()
print('取另一个小数f：',f)
print('对Π取近似值：',e.round())
print('对f取近似值：',f.round())

print('---------------------------------------------------------------------------------------------------------------')
# 这里用梯度举例，是因为在深度学习中，梯度很容易出现过大（100）或者过小（0.01）。一般认为梯度在1-10效果最好。
grad = (torch.rand(5,5)*15).cuda()  # 生成一个二维5*5，基于0-1均匀分布的张量，并且把所有元素扩大15倍。
print('输出随机矩阵张量grad：',grad)  # 输出生成的模拟梯度张量
print('grad元素最大值：',grad.max())  # 输出元素的最大值
print('grad元素中间值：',grad.median())   # 输出元素的中位数
grad_minlim = grad.clamp(10)  # 将grad中的最小值变为10（即把grad中比10小的值变为10）
print('将最小值限制为10的grad：',grad_minlim)  # 输出结果
grad_m = grad.clamp(1,10)  # 将grad中元素的最小值变为1，最大值变为10.
print('将最小值限制为1，最大值限制为10的grad：',grad_m)  # 输出结果

print('---------------------------------------------------------------------------------------------------------------')

print('程序结束时间：',time.strftime('%Y-%m-%d %H:%M:%S'))  # 记录程序结束时间